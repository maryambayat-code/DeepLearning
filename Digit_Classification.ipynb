{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "N7W7CpjLob0N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "digits=load_digits()\n",
        "digits.keys()\n",
        "print(digits['DESCR'])"
      ],
      "metadata": {
        "id": "hbMYOeupoaJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(np.reshape(digits['data'][0,:],(8,8)))\n",
        "plt.axis('off')\n",
        "print(digits['target'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "zhzNUcqNv1Sf",
        "outputId": "3c3b2fcf-45cb-45a4-b55c-5ae60da6a20f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEHUlEQVR4nO3dP2veZRiG4Sc1ikutYkEHFxE6CAWlOBScLIJOumgzORZcGxyEin6EboK7KLoqIq5CwFKRutShs3+xFBxEYl6/wEvHOyfxOMZkuH7LmQey3DubzWYBPaeO+wOA7cQJUeKEKHFClDghavd+v3z51Bsn8l+5f1y5OLr3zv6nY1vv3XxtbOvc1Z/Htg5/+XVsa9o3R5/vbPu5lxOixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR9z3HcFJNnkdYa62903fHtq4/+tfY1pfffz22deGDt8e21lrr7EcHo3vbeDkhSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQlTnHcPjShbGtvdM/jG2ttdarr+yNbZ25dXts681vL41t/fn8v2Nba611dnRtOy8nRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUZlbKX8/Pvcp1347P7a11lpHg/dLJt348Znj/oQTzcsJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEqM45hsfm/k58fHBxbGuttc6t70b3puye+Wds6/DeQ2NbFV5OiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRGXOMTx892hs64Xzd8a21lrr3uDW7pNPjG1dfvbm2NZnX704tlXh5YQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUJU5hzDIz/NHS14/6kvxrbWWuutK1fHth58/fexrUlPv3tw3J8wzssJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFCVOZWytGt22Nblz/cH9taa61r+5+MbV2/c2ls68ZzD4xt/R95OSFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBC1s9lsjvsbgC28nBAlTogSJ0SJE6LECVHihKj/AMJTSyrsVbDvAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self,input_size=64,output_size=10,layers=[30,30]):\n",
        "    super().__init__()\n",
        "    self.fc1=nn.Linear(input_size,layers[0])####input layer 1\n",
        "    self.fc2=nn.Linear(layers[0],layers[1])###hidden layer\n",
        "    self.fc3=nn.Linear(layers[1],output_size)###output layer\n",
        "\n",
        "  def forward(self,X):\n",
        "    X=F.relu(self.fc1(X))###usingActivation Fun On Input\n",
        "    X=F.relu(self.fc2(X))\n",
        "    X=F.log_softmax(self.fc3(X),dim=1)###dim=1 means I want to use softmax in relation with columns\n",
        "    return X\n"
      ],
      "metadata": {
        "id": "R9UGcQAjy_XB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= MLP()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7Dy6Drl3IcR",
        "outputId": "ad29fcdb-3d93-4bb1-bd9c-aaf3470d43c2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (fc1): Linear(in_features=64, out_features=30, bias=True)\n",
            "  (fc2): Linear(in_features=30, out_features=30, bias=True)\n",
            "  (fc3): Linear(in_features=30, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criteria=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=0.001,betas=[0.9,0.999])"
      ],
      "metadata": {
        "id": "fsHcFSozopH1"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train,X_Test,Y_Train,Y_Test= train_test_split(digits['data'],digits['target'],test_size=0.2)"
      ],
      "metadata": {
        "id": "y0fpvaf55aiL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train.shape , Y_Train.shape "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG5yAGdt58ed",
        "outputId": "d9740190-231e-4b95-bb84-bedd6adc615c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1437, 64), (1437,))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_Test.shape ,Y_Test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W62RXKF76FGB",
        "outputId": "7537478f-0323-4a69-a04c-11deb2ed9f3c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((360, 64), (360,))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train=torch.from_numpy(X_Train)\n",
        "X_Train=X_Train.to(torch.float32)\n",
        "X_Test=torch.from_numpy(X_Test)\n",
        "X_Test=X_Test.to(torch.float32)\n",
        "\n",
        "Y_Train=torch.from_numpy(Y_Train)\n",
        "Y_Test=torch.from_numpy(Y_Test)"
      ],
      "metadata": {
        "id": "EiQHa8R36KJq"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "  output = model(X_Train)\n",
        "  loss=criteria(output,Y_Train)\n",
        "  pred=torch.max(output.data,dim=1)[1]\n",
        "  acc_Train=accuracy_score(y_true=Y_Train,y_pred=pred)\n",
        "  optimizer.zero_grad()###for forgetting the previous gradient\n",
        "  loss.backward()\n",
        "  optimizer.step()### for doing one step of optimization\n",
        "\n",
        "  print('epoch:{}, Loss:{} ,Acc_Train:'.format(epoch, np.round(loss.item(),3)), acc_Train)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AaqhwUw_zSl",
        "outputId": "cd8596a1-5936-4bfb-87ef-fbe14ab2d3da"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0, Loss:0.032 ,Acc_Train: 0.9965205288796103\n",
            "epoch:1, Loss:0.031 ,Acc_Train: 0.9965205288796103\n",
            "epoch:2, Loss:0.031 ,Acc_Train: 0.9965205288796103\n",
            "epoch:3, Loss:0.031 ,Acc_Train: 0.9965205288796103\n",
            "epoch:4, Loss:0.03 ,Acc_Train: 0.9965205288796103\n",
            "epoch:5, Loss:0.03 ,Acc_Train: 0.9965205288796103\n",
            "epoch:6, Loss:0.03 ,Acc_Train: 0.9965205288796103\n",
            "epoch:7, Loss:0.029 ,Acc_Train: 0.9965205288796103\n",
            "epoch:8, Loss:0.029 ,Acc_Train: 0.9965205288796103\n",
            "epoch:9, Loss:0.029 ,Acc_Train: 0.9965205288796103\n",
            "epoch:10, Loss:0.028 ,Acc_Train: 0.9965205288796103\n",
            "epoch:11, Loss:0.028 ,Acc_Train: 0.9965205288796103\n",
            "epoch:12, Loss:0.028 ,Acc_Train: 0.9965205288796103\n",
            "epoch:13, Loss:0.028 ,Acc_Train: 0.9965205288796103\n",
            "epoch:14, Loss:0.027 ,Acc_Train: 0.9965205288796103\n",
            "epoch:15, Loss:0.027 ,Acc_Train: 0.9965205288796103\n",
            "epoch:16, Loss:0.027 ,Acc_Train: 0.9965205288796103\n",
            "epoch:17, Loss:0.026 ,Acc_Train: 0.9965205288796103\n",
            "epoch:18, Loss:0.026 ,Acc_Train: 0.9965205288796103\n",
            "epoch:19, Loss:0.026 ,Acc_Train: 0.9965205288796103\n",
            "epoch:20, Loss:0.026 ,Acc_Train: 0.9965205288796103\n",
            "epoch:21, Loss:0.025 ,Acc_Train: 0.9965205288796103\n",
            "epoch:22, Loss:0.025 ,Acc_Train: 0.9965205288796103\n",
            "epoch:23, Loss:0.025 ,Acc_Train: 0.9965205288796103\n",
            "epoch:24, Loss:0.025 ,Acc_Train: 0.9965205288796103\n",
            "epoch:25, Loss:0.024 ,Acc_Train: 0.9972164231036882\n",
            "epoch:26, Loss:0.024 ,Acc_Train: 0.9972164231036882\n",
            "epoch:27, Loss:0.024 ,Acc_Train: 0.9972164231036882\n",
            "epoch:28, Loss:0.024 ,Acc_Train: 0.9972164231036882\n",
            "epoch:29, Loss:0.023 ,Acc_Train: 0.9986082115518441\n",
            "epoch:30, Loss:0.023 ,Acc_Train: 0.9986082115518441\n",
            "epoch:31, Loss:0.023 ,Acc_Train: 0.9986082115518441\n",
            "epoch:32, Loss:0.023 ,Acc_Train: 0.9986082115518441\n",
            "epoch:33, Loss:0.022 ,Acc_Train: 0.9986082115518441\n",
            "epoch:34, Loss:0.022 ,Acc_Train: 0.9986082115518441\n",
            "epoch:35, Loss:0.022 ,Acc_Train: 0.9986082115518441\n",
            "epoch:36, Loss:0.022 ,Acc_Train: 0.9986082115518441\n",
            "epoch:37, Loss:0.022 ,Acc_Train: 0.9986082115518441\n",
            "epoch:38, Loss:0.021 ,Acc_Train: 0.9986082115518441\n",
            "epoch:39, Loss:0.021 ,Acc_Train: 0.9986082115518441\n",
            "epoch:40, Loss:0.021 ,Acc_Train: 0.9986082115518441\n",
            "epoch:41, Loss:0.021 ,Acc_Train: 0.9986082115518441\n",
            "epoch:42, Loss:0.02 ,Acc_Train: 0.9986082115518441\n",
            "epoch:43, Loss:0.02 ,Acc_Train: 0.9986082115518441\n",
            "epoch:44, Loss:0.02 ,Acc_Train: 0.9986082115518441\n",
            "epoch:45, Loss:0.02 ,Acc_Train: 0.9986082115518441\n",
            "epoch:46, Loss:0.02 ,Acc_Train: 0.9986082115518441\n",
            "epoch:47, Loss:0.02 ,Acc_Train: 0.9986082115518441\n",
            "epoch:48, Loss:0.019 ,Acc_Train: 0.9986082115518441\n",
            "epoch:49, Loss:0.019 ,Acc_Train: 0.9986082115518441\n",
            "epoch:50, Loss:0.019 ,Acc_Train: 0.9986082115518441\n",
            "epoch:51, Loss:0.019 ,Acc_Train: 0.9986082115518441\n",
            "epoch:52, Loss:0.019 ,Acc_Train: 0.9986082115518441\n",
            "epoch:53, Loss:0.018 ,Acc_Train: 0.9986082115518441\n",
            "epoch:54, Loss:0.018 ,Acc_Train: 0.9986082115518441\n",
            "epoch:55, Loss:0.018 ,Acc_Train: 0.9986082115518441\n",
            "epoch:56, Loss:0.018 ,Acc_Train: 0.9986082115518441\n",
            "epoch:57, Loss:0.018 ,Acc_Train: 0.9986082115518441\n",
            "epoch:58, Loss:0.018 ,Acc_Train: 0.9986082115518441\n",
            "epoch:59, Loss:0.017 ,Acc_Train: 0.9993041057759221\n",
            "epoch:60, Loss:0.017 ,Acc_Train: 0.9993041057759221\n",
            "epoch:61, Loss:0.017 ,Acc_Train: 0.9993041057759221\n",
            "epoch:62, Loss:0.017 ,Acc_Train: 1.0\n",
            "epoch:63, Loss:0.017 ,Acc_Train: 1.0\n",
            "epoch:64, Loss:0.017 ,Acc_Train: 1.0\n",
            "epoch:65, Loss:0.016 ,Acc_Train: 1.0\n",
            "epoch:66, Loss:0.016 ,Acc_Train: 1.0\n",
            "epoch:67, Loss:0.016 ,Acc_Train: 1.0\n",
            "epoch:68, Loss:0.016 ,Acc_Train: 1.0\n",
            "epoch:69, Loss:0.016 ,Acc_Train: 1.0\n",
            "epoch:70, Loss:0.016 ,Acc_Train: 1.0\n",
            "epoch:71, Loss:0.016 ,Acc_Train: 1.0\n",
            "epoch:72, Loss:0.015 ,Acc_Train: 1.0\n",
            "epoch:73, Loss:0.015 ,Acc_Train: 1.0\n",
            "epoch:74, Loss:0.015 ,Acc_Train: 1.0\n",
            "epoch:75, Loss:0.015 ,Acc_Train: 1.0\n",
            "epoch:76, Loss:0.015 ,Acc_Train: 1.0\n",
            "epoch:77, Loss:0.015 ,Acc_Train: 1.0\n",
            "epoch:78, Loss:0.015 ,Acc_Train: 1.0\n",
            "epoch:79, Loss:0.014 ,Acc_Train: 1.0\n",
            "epoch:80, Loss:0.014 ,Acc_Train: 1.0\n",
            "epoch:81, Loss:0.014 ,Acc_Train: 1.0\n",
            "epoch:82, Loss:0.014 ,Acc_Train: 1.0\n",
            "epoch:83, Loss:0.014 ,Acc_Train: 1.0\n",
            "epoch:84, Loss:0.014 ,Acc_Train: 1.0\n",
            "epoch:85, Loss:0.014 ,Acc_Train: 1.0\n",
            "epoch:86, Loss:0.014 ,Acc_Train: 1.0\n",
            "epoch:87, Loss:0.014 ,Acc_Train: 1.0\n",
            "epoch:88, Loss:0.013 ,Acc_Train: 1.0\n",
            "epoch:89, Loss:0.013 ,Acc_Train: 1.0\n",
            "epoch:90, Loss:0.013 ,Acc_Train: 1.0\n",
            "epoch:91, Loss:0.013 ,Acc_Train: 1.0\n",
            "epoch:92, Loss:0.013 ,Acc_Train: 1.0\n",
            "epoch:93, Loss:0.013 ,Acc_Train: 1.0\n",
            "epoch:94, Loss:0.013 ,Acc_Train: 1.0\n",
            "epoch:95, Loss:0.013 ,Acc_Train: 1.0\n",
            "epoch:96, Loss:0.012 ,Acc_Train: 1.0\n",
            "epoch:97, Loss:0.012 ,Acc_Train: 1.0\n",
            "epoch:98, Loss:0.012 ,Acc_Train: 1.0\n",
            "epoch:99, Loss:0.012 ,Acc_Train: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###findingThePerformanceOf Model\n",
        "out=model(X_Test)\n",
        "Y_pred=torch.max(out.data,dim=1)[1]\n",
        "print(accuracy_score(y_true=Y_Test,y_pred=Y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF-4AXZjDW5g",
        "outputId": "83326953-bb6e-4995-eaa8-76ee2600e0fc"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9666666666666667\n"
          ]
        }
      ]
    }
  ]
}